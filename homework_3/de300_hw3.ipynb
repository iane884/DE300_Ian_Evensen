{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 33.2M  100 33.2M    0     0  16.2M      0  0:00:02  0:00:02 --:--:-- 16.2M\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p dataset\n",
    "!curl -L https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/agnews_clean.csv \\\n",
    "     -o dataset/agnews_clean.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .master(\"local[*]\")\n",
    "         .appName(\"AG news\")\n",
    "         .getOrCreate()\n",
    "        )\n",
    "\n",
    "agnews = spark.read.csv(\"dataset/agnews_clean.csv\", inferSchema=True, header=True)\n",
    "\n",
    "# turning the second column from a string to an array\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "agnews = agnews.withColumn('filtered', F.from_json('filtered', ArrayType(StringType())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------+\n",
      "|_c0|                      filtered|\n",
      "+---+------------------------------+\n",
      "|  0|[wall, st, bears, claw, bac...|\n",
      "|  1|[carlyle, looks, toward, co...|\n",
      "|  2|[oil, economy, cloud, stock...|\n",
      "|  3|[iraq, halts, oil, exports,...|\n",
      "|  4|[oil, prices, soar, time, r...|\n",
      "+---+------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# each row contains the document id and a list of filtered words\n",
    "agnews.show(5, truncate=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents: 127600\n"
     ]
    }
   ],
   "source": [
    "num_docs = agnews.count()\n",
    "print(\"Total documents:\", num_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. tf-idf definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do we need to calculate?\n",
    "\n",
    "- For each $d$, the counts of $t$\n",
    "- For each $d$, the counts of words,\n",
    "- For each $t$, the counts of $d$ that contains $t$.\n",
    "- what should be returned if we only want to know if the document contains $t$ of not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build rdd (docId, tokens)\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "doc_rdd = (\n",
    "    agnews\n",
    "    .select(col(\"_c0\").alias(\"docId\"),\n",
    "            col(\"filtered\").alias(\"tokens\"))\n",
    "    .rdd\n",
    "    .map(lambda row: (row.docId, row.tokens))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((0, 'claw'), 1),\n",
       " ((0, 'green'), 1),\n",
       " ((1, 'looks'), 1),\n",
       " ((1, 'investment'), 1),\n",
       " ((1, 'quietly'), 1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For each $d$, the counts of $t$\n",
    "t_counts = (\n",
    "    doc_rdd\n",
    "    .flatMap(lambda doc: [((doc[0], t), 1) for t in doc[1]])\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    ")\n",
    "\n",
    "t_counts.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 18), (1, 27), (2, 24), (3, 28), (4, 30)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For each $d$, the counts of words\n",
    "word_counts = (\n",
    "    doc_rdd\n",
    "    .map(lambda doc: (doc[0], len(doc[1])))\n",
    ")\n",
    "\n",
    "word_counts.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('latest', 1820),\n",
       " ('17', 1218),\n",
       " ('japan', 1948),\n",
       " ('corporate', 809),\n",
       " ('shuts', 82)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For each $t$, the counts of $d$ that contains $t$\n",
    "\n",
    "# get (term, id) from t_counts\n",
    "t_docs = t_counts.map(lambda kv: (kv[0][1], kv[0][0]))\n",
    "\n",
    "# remove duplicate term counts\n",
    "distinct_t_docs = t_docs.distinct()\n",
    "\n",
    "# count how many $d$ each term is in\n",
    "d_counts = (\n",
    "    distinct_t_docs\n",
    "    .map(lambda td: (td[0], 1))\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    ")\n",
    "\n",
    "d_counts.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((0, 'claw'), 1),\n",
       " ((0, 'green'), 1),\n",
       " ((1, 'looks'), 1),\n",
       " ((1, 'investment'), 1),\n",
       " ((1, 'quietly'), 1),\n",
       " ((1, 'bets'), 1),\n",
       " ((2, 'cloud'), 1),\n",
       " ((2, 'outlook'), 1),\n",
       " ((2, 'doldrums'), 1),\n",
       " ((3, 'iraq'), 1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what should be returned if we only want to know if the document contains $t$ of not\n",
    "\n",
    "# d_contains_t = RDD[((id, term), 1)]\n",
    "d_contains_t = t_counts.map(lambda kv: (kv[0], 1))\n",
    "\n",
    "d_contains_t.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "1. Design the MapReduce functions for calculating the tf-idf measure.\n",
    "2. Calculate tf-idf measure for each row in the agnews_clean.csv. Save the measures in a new column.\n",
    "3. Print out the tf-idf measure for the first 5 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((0, 'claw'), 0.05555555555555555),\n",
       " ((0, 'green'), 0.05555555555555555),\n",
       " ((0, 'wall'), 0.1111111111111111),\n",
       " ((0, 'cynics'), 0.05555555555555555),\n",
       " ((0, 'back'), 0.05555555555555555)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute tf\n",
    "tf = (t_counts\n",
    "      .map(lambda kv: (kv[0][0], (kv[0][1], kv[1])))\n",
    "      .join(word_counts)\n",
    "      .map(lambda rec: (\n",
    "          (rec[0], rec[1][0][0]),\n",
    "          rec[1][0][1] / rec[1][1]\n",
    "      )))\n",
    "\n",
    "tf.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('latest', 4.250063869821986),\n",
       " ('17', 4.651690201622984),\n",
       " ('japan', 4.1820971656903465),\n",
       " ('corporate', 5.060856732834335),\n",
       " ('shuts', 7.349936402628574)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute idf\n",
    "import math\n",
    "\n",
    "idf = (d_counts.map(lambda kv: (kv[0], math.log(num_docs / kv[1]))))\n",
    "\n",
    "idf.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((32, 'push'), 0.232878964391475),\n",
       " ((3808, 'push'), 0.1506863887238956),\n",
       " ((10928, 'push'), 0.3013727774477912),\n",
       " ((10976, 'push'), 0.24396843888630712),\n",
       " ((11536, 'push'), 0.204933488664498),\n",
       " ((12864, 'push'), 0.3202085760382781),\n",
       " ((21536, 'push'), 0.16526894247136936),\n",
       " ((26320, 'push'), 0.170777907220415),\n",
       " ((27904, 'push'), 0.1766668005728431),\n",
       " ((27984, 'push'), 0.1506863887238956)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute tfâ€“idf measures\n",
    "\n",
    "# rekey tf by term to join on idf\n",
    "rekeyed_tf = tf.map(lambda kv: (kv[0][1], (kv[0][0], kv[1])))  \n",
    "\n",
    "# join on idf & multiply\n",
    "tf_idf = (\n",
    "    rekeyed_tf\n",
    "      .join(idf)\n",
    "      .map(lambda kv: (\n",
    "          (kv[1][0][0], kv[0]),\n",
    "          kv[1][0][1] * kv[1][1]\n",
    "      ))\n",
    ")\n",
    "\n",
    "tf_idf.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+--------------------+-------+-------------------+\n",
      "|docId|_c0|            filtered|   term|              tfidf|\n",
      "+-----+---+--------------------+-------+-------------------+\n",
      "|    0|  0|[wall, st, bears,...|   wall| 0.5115985326511431|\n",
      "|    0|  0|[wall, st, bears,...|reuters|0.24754017186645658|\n",
      "|    0|  0|[wall, st, bears,...|  short| 0.2773120373951269|\n",
      "|    0|  0|[wall, st, bears,...|  ultra| 0.4125512394225831|\n",
      "|    0|  0|[wall, st, bears,...|  bears| 0.3372044607529448|\n",
      "+-----+---+--------------------+-------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# save measures in new column\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# create (docId, term, tfidf) rows\n",
    "rows = tf_idf.map(lambda kv: Row(docId=kv[0][0], term=kv[0][1], tfidf=kv[1]))\n",
    "tf_idf_df = spark.createDataFrame(rows)\n",
    "\n",
    "# join onto original dataframe\n",
    "agnews_with_id = agnews.withColumn(\"docId\", F.monotonically_increasing_id())\n",
    "\n",
    "result = (agnews_with_id.join(tf_idf_df, on=\"docId\", how=\"left\"))\n",
    "\n",
    "result.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+------------------+\n",
      "|docId|     term|             tfidf|\n",
      "+-----+---------+------------------+\n",
      "|    0|   cynics| 0.563734318747707|\n",
      "|    0|     wall|0.5115985326511431|\n",
      "|    0|     claw| 0.499114829314058|\n",
      "|    0|dwindling|0.4572386180709258|\n",
      "|    0|  sellers|0.4468379768438066|\n",
      "+-----+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print out the tf-idf measure for the first 5 documents\n",
    "result.filter(\"docId < 5\").select(\"docId\", \"term\", \"tfidf\").orderBy(\"docId\", F.desc(\"tfidf\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. SVM objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "1. Design the MapReduce functions required to calculate the loss function.\n",
    "2. Using these functions, create a function loss_SVM(w, b, X, y) to calculate the SVM objective for a given choice of w, b with data stored in X, y.\n",
    "3. You are given the following dataset data_for_svm.csv, where the first 64 columns contain X and the last column contains y. Using the weights and bias provided in w.csv and bias.csv, calculate the objective value.\n",
    "4. Design the MapReduce function required to make prediction. Predict for all of the data using the provided weights and bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  1391  100  1391    0     0   4117      0 --:--:-- --:--:-- --:--:--  4115\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100    22  100    22    0     0     74      0 --:--:-- --:--:-- --:--:--    74\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 61.9M  100 61.9M    0     0  18.7M      0  0:00:03  0:00:03 --:--:-- 18.7M\n"
     ]
    }
   ],
   "source": [
    "!curl https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/w.csv -O\n",
    "!curl https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/bias.csv -O\n",
    "!curl https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/data_for_svm.csv -O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([-1,\n",
       "   -1,\n",
       "   1,\n",
       "   1,\n",
       "   -1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   -1,\n",
       "   1,\n",
       "   1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   1,\n",
       "   -1,\n",
       "   1,\n",
       "   -1,\n",
       "   1,\n",
       "   -1,\n",
       "   1,\n",
       "   1,\n",
       "   -1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   -1,\n",
       "   1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   -1,\n",
       "   1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   1,\n",
       "   -1,\n",
       "   1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   1],\n",
       "  -1),\n",
       " ([1,\n",
       "   1,\n",
       "   -1,\n",
       "   1,\n",
       "   -1,\n",
       "   -1,\n",
       "   1,\n",
       "   1,\n",
       "   -1,\n",
       "   1,\n",
       "   1,\n",
       "   -1,\n",
       "   1,\n",
       "   -1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   -1,\n",
       "   1,\n",
       "   -1,\n",
       "   -1,\n",
       "   1,\n",
       "   -1,\n",
       "   1,\n",
       "   -1,\n",
       "   -1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   -1,\n",
       "   -1,\n",
       "   1,\n",
       "   1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   1,\n",
       "   1,\n",
       "   -1],\n",
       "  1),\n",
       " ([1,\n",
       "   1,\n",
       "   1,\n",
       "   -1,\n",
       "   1,\n",
       "   1,\n",
       "   -1,\n",
       "   1,\n",
       "   -1,\n",
       "   1,\n",
       "   -1,\n",
       "   -1,\n",
       "   1,\n",
       "   -1,\n",
       "   -1,\n",
       "   1,\n",
       "   -1,\n",
       "   -1,\n",
       "   1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   1,\n",
       "   -1,\n",
       "   1,\n",
       "   1,\n",
       "   -1,\n",
       "   -1,\n",
       "   1,\n",
       "   1,\n",
       "   -1,\n",
       "   -1,\n",
       "   1,\n",
       "   1,\n",
       "   -1,\n",
       "   1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   -1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  1)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load svm data into rdd\n",
    "\n",
    "svm_df = spark.read.csv(\"data_for_svm.csv\", header=False, inferSchema=True)\n",
    "\n",
    "# convert to rdd with (features, y) rows\n",
    "svm_rdd = svm_df.rdd.map(\n",
    "    lambda row: ([row[i] for i in range(len(row) - 1)], row[-1])\n",
    ")\n",
    "\n",
    "svm_rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# broad cast weights and bias\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# read weight vector from w.csv\n",
    "w = pd.read_csv(\"w.csv\", header=None).iloc[:,0].tolist()\n",
    "\n",
    "# read bias scalar from bias.csv\n",
    "b = float(pd.read_csv(\"bias.csv\", header=None).iloc[0,0])\n",
    "\n",
    "# broadcast weights and biases together\n",
    "wb = spark.sparkContext.broadcast((w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create loss_SVM(w, b, X, y) function\n",
    "\n",
    "\n",
    "def loss_SVM(data_rdd, lam):\n",
    "\n",
    "    # get total # of examples\n",
    "    num_examples = data_rdd.count()\n",
    "\n",
    "    # get broadcasted (w, b) tuple\n",
    "    w_b = wb.value\n",
    "    \n",
    "    # calculate hinge loss for each example and sum\n",
    "    # for each (x, y):\n",
    "    # - margin = y( wx + b)\n",
    "    # - hinge = max(0, 1 - margin)\n",
    "    hinge_sum = data_rdd.map(lambda xy: max(\n",
    "        0.0,\n",
    "        1 - xy[1] * (sum(w_i * x_i for w_i, x_i in zip(w_b[0], xy[0])) + w_b[1]\n",
    "        )\n",
    "    )).reduce(lambda a, b: a + b)\n",
    "    \n",
    "    # compute L2 regularization term\n",
    "    l2_reg = lam * sum(w_i * w_i for w_i in w_b[0])\n",
    "\n",
    "    # return objective value (regularization + mean hinge loss)\n",
    "    return l2_reg + hinge_sum / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM objective: 1.000001871756999\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "print(\"SVM objective:\", loss_SVM(svm_rdd, lam=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# design MapReduce function\n",
    "def map_reduce(data_rdd):\n",
    "    # get broadcasted (w, b) tuple\n",
    "    w_b = wb.value\n",
    "\n",
    "    # compute predicted label for each example\n",
    "    return data_rdd.map(\n",
    "        lambda xy: (\n",
    "            xy[1],\n",
    "            1 if (sum(w_i * x_i for w_i, x_i in zip(w_b[0], xy[0])) + w_b[1]) >= 0\n",
    "              else -1\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-1, 1),\n",
       " (1, -1),\n",
       " (1, -1),\n",
       " (1, -1),\n",
       " (-1, -1),\n",
       " (-1, -1),\n",
       " (-1, -1),\n",
       " (1, 1),\n",
       " (-1, -1),\n",
       " (1, -1)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "predictions = map_reduce(svm_rdd)\n",
    "predictions.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "1bd7958cf2faeb1cfe3a2d6ba5a1b6ee7f65810dd9ddc3f16e2e54b13741754a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
